{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc88545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FCCNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_output_classes, num_filters, num_layers, use_bias=False):\n",
    "        \"\"\"\n",
    "        Initializes a fully connected network similar to the ones implemented previously in the MLP package.\n",
    "        :param input_shape: The shape of the inputs going in to the network.\n",
    "        :param num_output_classes: The number of outputs the network should have (for classification those would be the number of classes)\n",
    "        :param num_filters: Number of filters used in every fcc layer.\n",
    "        :param num_layers: Number of fcc layers (excluding dim reduction stages)\n",
    "        :param use_bias: Whether our fcc layers will use a bias.\n",
    "        \"\"\"\n",
    "        super(FCCNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_layers = num_layers\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        print(\"Building basic block of FCCNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))\n",
    "\n",
    "        out = x\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        # flatten inputs to shape (b, -1) where -1 is the dim resulting from multiplying the\n",
    "        # shapes of all dimensions after the 0th dim\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_dict['fcc_{}'.format(i)] = nn.Linear(in_features=out.shape[1],  # initialize a fcc layer\n",
    "                                                            out_features=self.num_filters,\n",
    "                                                            bias=self.use_bias)\n",
    "\n",
    "            out = self.layer_dict['fcc_{}'.format(i)](out)  # apply ith fcc layer to the previous layers outputs\n",
    "            out = F.relu(out)  # apply a ReLU on the outputs\n",
    "\n",
    "        self.logits_linear_layer = nn.Linear(in_features=out.shape[1],  # initialize the prediction output linear layer\n",
    "                                             out_features=self.num_output_classes,\n",
    "                                             bias=self.use_bias)\n",
    "        out = self.logits_linear_layer(out)  # apply the layer to the previous layer's outputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward prop data through the network and return the preds\n",
    "        :param x: Input batch x a batch of shape batch number of samples, each of any dimensionality.\n",
    "        :return: preds of shape (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        # flatten inputs to shape (b, -1) where -1 is the dim resulting from multiplying the\n",
    "        # shapes of all dimensions after the 0th dim\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            out = self.layer_dict['fcc_{}'.format(i)](out)  # apply ith fcc layer to the previous layers outputs\n",
    "            out = F.relu(out)  # apply a ReLU on the outputs\n",
    "\n",
    "        out = self.logits_linear_layer(out)  # apply the layer to the previous layer's outputs\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initializes the networks parameters\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            item.reset_parameters()\n",
    "\n",
    "        self.logits_linear_layer.reset_parameters()\n",
    "\n",
    "\n",
    "class EmptyBlock(nn.Module):\n",
    "    def __init__(self, input_shape=None, num_filters=None, kernel_size=None, padding=None, bias=None, dilation=None,\n",
    "                 reduction_factor=None):\n",
    "        super(EmptyBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        self.layer_dict['Identity'] = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['Identity'].forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EntryConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(EntryConvolutionalBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
    "        out = F.leaky_relu(self.layer_dict['bn_0'].forward(out))\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(self.layer_dict['bn_0'].forward(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalProcessingBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvolutionalProcessingBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalDimensionalityReductionBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation, reduction_factor):\n",
    "        super(ConvolutionalDimensionalityReductionBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_output_classes, num_filters,\n",
    "                 num_blocks_per_stage, num_stages, use_bias=False, processing_block_type=ConvolutionalProcessingBlock,\n",
    "                 dimensionality_reduction_block_type=ConvolutionalDimensionalityReductionBlock):\n",
    "        \"\"\"\n",
    "        Initializes a convolutional network module\n",
    "        :param input_shape: The shape of the tensor to be passed into this network\n",
    "        :param num_output_classes: Number of output classes\n",
    "        :param num_filters: Number of filters per convolutional layer\n",
    "        :param num_blocks_per_stage: Number of blocks per \"stage\". Each block is composed of 2 convolutional layers.\n",
    "        :param num_stages: Number of stages in a network. A stage is defined as a sequence of layers within which the\n",
    "        data dimensionality remains constant in the spacial axis (h, w) and can change in the channel axis. After each stage\n",
    "        there exists a dimensionality reduction stage, composed of two convolutional layers and an avg pooling layer.\n",
    "        :param use_bias: Whether to use biases in our convolutional layers\n",
    "        :param processing_block_type: Type of processing block to use within our stages\n",
    "        :param dimensionality_reduction_block_type: Type of dimensionality reduction block to use after each stage in our network\n",
    "        \"\"\"\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_blocks_per_stage = num_blocks_per_stage\n",
    "        self.num_stages = num_stages\n",
    "        self.processing_block_type = processing_block_type\n",
    "        self.dimensionality_reduction_block_type = dimensionality_reduction_block_type\n",
    "\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        \"\"\"\n",
    "        Builds network whilst automatically inferring shapes of layers.\n",
    "        \"\"\"\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        print(\"Building basic block of ConvolutionalNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))  # create dummy inputs to be used to infer shapes of layers\n",
    "\n",
    "        out = x\n",
    "        self.layer_dict['input_conv'] = EntryConvolutionalBlock(input_shape=out.shape, num_filters=self.num_filters,\n",
    "                                                                kernel_size=3, padding=1, bias=self.use_bias,\n",
    "                                                                dilation=1)\n",
    "        out = self.layer_dict['input_conv'].forward(out)\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        for i in range(self.num_stages):  # for number of layers times\n",
    "            for j in range(self.num_blocks_per_stage):\n",
    "                self.layer_dict['block_{}_{}'.format(i, j)] = self.processing_block_type(input_shape=out.shape,\n",
    "                                                                                         num_filters=self.num_filters,\n",
    "                                                                                         bias=self.use_bias,\n",
    "                                                                                         kernel_size=3, dilation=1,\n",
    "                                                                                         padding=1)\n",
    "                out = self.layer_dict['block_{}_{}'.format(i, j)].forward(out)\n",
    "            self.layer_dict['reduction_block_{}'.format(i)] = self.dimensionality_reduction_block_type(\n",
    "                input_shape=out.shape,\n",
    "                num_filters=self.num_filters, bias=True,\n",
    "                kernel_size=3, dilation=1,\n",
    "                padding=1,\n",
    "                reduction_factor=2)\n",
    "            out = self.layer_dict['reduction_block_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, out.shape[-1])\n",
    "        print('shape before final linear layer', out.shape)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        self.logit_linear_layer = nn.Linear(in_features=out.shape[1],  # add a linear layer\n",
    "                                            out_features=self.num_output_classes,\n",
    "                                            bias=True)\n",
    "        out = self.logit_linear_layer(out)  # apply linear layer on flattened inputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propages the network given an input batch\n",
    "        :param x: Inputs x (b, c, h, w)\n",
    "        :return: preds (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        out = self.layer_dict['input_conv'].forward(out)\n",
    "        for i in range(self.num_stages):  # for number of layers times\n",
    "            for j in range(self.num_blocks_per_stage):\n",
    "                out = self.layer_dict['block_{}_{}'.format(i, j)].forward(out)\n",
    "            out = self.layer_dict['reduction_block_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, out.shape[-1])\n",
    "        out = out.view(out.shape[0], -1)  # flatten outputs from (b, c, h, w) to (b, c*h*w)\n",
    "        out = self.logit_linear_layer(out)  # pass through a linear layer to get logits/preds\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the network parameters.\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            try:\n",
    "                item.reset_parameters()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.logit_linear_layer.reset_parameters()\n",
    "\n",
    "# Processing batch normalisation block\n",
    "class ConvolutionalProcessingBlockBatchNormalisation(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvolutionalProcessingBlockBatchNormalisation, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(num_features=out.shape[1])  # Batch normalisation\n",
    "        \n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['bn_0'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['bn_1'] = nn.BatchNorm2d(num_features=out.shape[1])  # Batch normalisation\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['bn_1'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['bn_0'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['bn_1'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Processing batch normalisation and residual connection block\n",
    "class ConvolutionalProcessingBlockBatchNormalisationResidualConnection(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvolutionalProcessingBlockBatchNormalisationResidualConnection, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(num_features=out.shape[1]) # Batch normalisation\n",
    "        \n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['bn_0'].forward(out) # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['bn_1'] = nn.BatchNorm2d(num_features=out.shape[1]) # Batch normalisation\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['bn_1'].forward(out) # Batch normalisation\n",
    "        out = F.leaky_relu(out + x) # Residual connection\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['bn_0'].forward(out) # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['bn_1'].forward(out) # Batch normalisation\n",
    "        out = F.leaky_relu(out + x) # Residual connection\n",
    "\n",
    "        return out\n",
    "\n",
    "# Dimensionality reduction batch normalisation block\n",
    "class ConvolutionalDimensionalityReductionBlockBatchNormalisation(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation, reduction_factor):\n",
    "        super(ConvolutionalDimensionalityReductionBlockBatchNormalisation, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(num_features=out.shape[1])  # Batch normalisation\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['bn_0'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['bn_1'] = nn.BatchNorm2d(num_features=out.shape[1])  # Batch normalisation\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['bn_1'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['bn_0'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['bn_1'].forward(out)  # Batch normalisation\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33871725",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 16, 64, 64)\n",
    "num_filters = 16\n",
    "bias = True\n",
    "kernel_size = 3\n",
    "dilation = 1\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec320558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.249s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 64, 64])\n",
      "torch.Size([100, 16, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fb8b05a7250>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unittest\n",
    "\n",
    "class TestConvolutionalProcessingBlockBatchNormalisation(unittest.TestCase):\n",
    "\n",
    "    def test_instantiation(self):\n",
    "        # Test instantiation with various parameters\n",
    "        try:\n",
    "            block = ConvolutionalProcessingBlockBatchNormalisation(\n",
    "                input_shape=input_shape, num_filters=num_filters, kernel_size=kernel_size, padding=padding, bias=bias, dilation=dilation)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Instantiation failed with error: {e}\")\n",
    "            \n",
    "    def test_forward_propagation(self):\n",
    "        # Define the parameters for the block\n",
    "        input_shape = (32, 16, 64, 64)  # Example input shape: batch_size=1, channels=3, height=64, width=64\n",
    "        num_filters = 16\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        bias = True\n",
    "        dilation = 1\n",
    "\n",
    "        # Initialize the block\n",
    "        block = ConvolutionalProcessingBlockBatchNormalisation(input_shape, num_filters, kernel_size, padding, bias, dilation)\n",
    "\n",
    "        # Create a sample input tensor\n",
    "        x = torch.randn(input_shape)\n",
    "\n",
    "        # Forward propagate the input through the block\n",
    "        output = block(x)\n",
    "\n",
    "        # Check if the output shape is as expected\n",
    "        expected_output_shape = (32, num_filters, 64, 64)  # Expected shape after processing\n",
    "        self.assertEqual(output.shape, expected_output_shape)\n",
    "\n",
    "unittest.main(argv=[''], exit=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6f513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
